{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "import inspect\n",
    "import os\n",
    "from ipywidgets import interact, IntSlider, Dropdown, VBox, Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple paramters and data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "x = np.array([-1/2, -1/6, 1/6, 1/2])\n",
    "y = np.array([1/4, 1/30, 1/30, 1/4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def indicator(condition):\n",
    "    return condition.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-layer ReLU neural network\n",
    "\n",
    "$ f_m(x;\\theta) = \\frac{1}{\\alpha_m} \\sum_{j=1}^m a_j(w_j x + b_j)_{+}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Lyaer ReLu Neural Network\n",
    "def twoLayerReluNet(alpha_m, a, w, b, x):\n",
    "    return 1/alpha_m * np.sum(a * relu(w * x + b))\n",
    "\n",
    "def network_risk(alpha_m, a, w, b, x, y):\n",
    "    # Compute wx_plus_b for all i and j\n",
    "    wx_plus_b = np.outer(x,w) + b  # Shape: (n, m)\n",
    "\n",
    "    # Apply ReLU\n",
    "    relu_mask = (wx_plus_b > 0).astype(float)  # Shape: (n, m)\n",
    "    relu = wx_plus_b * relu_mask  # Apply the mask\n",
    "\n",
    "      # Shape: (n,)\n",
    "    return np.mean((1/2)*((np.sum(a * relu, axis=1) / alpha_m) - y) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation of weights\n",
    "\n",
    "$a_j^0 \\sim \\mathcal{N}(0,\\beta^2_{1m})$ and $b_j^0, w_j^0 \\sim \\mathcal{N}(0,\\beta^2_{2m})$\n",
    "\n",
    "through normalisation we define\n",
    "\n",
    "$\\kappa := \\frac{\\beta_{1m} \\beta_{2m}}{\\alpha_m}$, $\\kappa' := \\frac{\\beta_{1m}}{\\beta_{2m}}$\n",
    "\n",
    "and\n",
    "\n",
    "$\\gamma := \\lim_{m \\rightarrow \\infty} \\frac{\\log{\\kappa}}{\\log{m}}$, $\\gamma' := \\lim_{m \\rightarrow \\infty} \\frac{\\log{\\kappa'}}{\\log{m}}$\n",
    "\n",
    "We take that parameters $a_m, \\beta_{1m}, \\beta_{2m}$ are taken to have a power-law relation to $m$ and so we are able to choose $\\gamma$ and $\\gamma'$ such that the $a$-lag training regime is used.\n",
    "\n",
    "<small>  T. Luo, Z.-Q. J. Xu, Z. Ma, and Y. Zhang. Phase diagram for two-layer relu neu\n",
    "ral networks at innite-width limit . In: Journal of Machine Learning Research 22.71\n",
    " (2021), pp. 147.</small>\n",
    "\n",
    "<small>  Z.Chen,Y.Li,T.Luo,Z.Zhou,andZ.-Q. J. Xu. Phase diagram of initial condensation\n",
    " for two-layer neural networks . In: arXiv preprint arXiv:2303.06561 (2023).</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We use the following to fix the values of gamma, gamma_prime and alpha_m\n",
    "gamma = 3/2\n",
    "gamma_prime = 0\n",
    "\n",
    "def generate_alpha_betas(m):\n",
    "    alpha_m = m**(gamma - gamma_prime)\n",
    "    kappa = m**(-gamma)\n",
    "    kappa_prime = m**(-gamma_prime)\n",
    "\n",
    "    beta_1m = math.sqrt(kappa*kappa_prime*alpha_m)\n",
    "    beta_2m = math.sqrt((kappa/kappa_prime)*alpha_m)\n",
    "\n",
    "    return alpha_m, beta_1m, beta_2m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient flow\n",
    "\n",
    "Empirical Risk equation - $\\mathcal{R}(\\theta) := \\frac{1}{2n} \\sum_{i=1}^m (f_m(x_i; \\theta) - y_i)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Typical parameter gradients\n",
    "def gradient_flow(a,w,b,alpha_m):\n",
    "    # Compute wx_plus_b for all i and j\n",
    "    wx_plus_b = np.outer(x,w) + b  # Shape: (n, m)\n",
    "\n",
    "    # Apply ReLU activation\n",
    "    relu_mask = (wx_plus_b > 0).astype(float)  # Shape: (n, m)\n",
    "    relu = wx_plus_b * relu_mask  # Apply the mask\n",
    "\n",
    "    # First term inside parentheses: (1 / alpha_m) * sum_j(a_j * ReLU_j)\n",
    "    term2 = (np.sum(a * relu, axis=1) / alpha_m) - y  # Shape: (n,)\n",
    "\n",
    "    # Full sum: sum relu multiplied with appropriate term2, over i\n",
    "    full_sum_a = np.sum(relu * term2[:, np.newaxis], axis=0)  # Shape: (m,)\n",
    "\n",
    "    # Multiply by aj * xi * 1{wx_plus_b > 0}\n",
    "    term1_w = (a * x[:, np.newaxis] * relu_mask)  # Shape: (n, m)\n",
    "    # Full sum: sum term1_w multiplied with appropriate term2, over i\n",
    "    full_sum_w = np.sum(term1_w * term2[:, np.newaxis], axis=0)  # Shape: (m,)\n",
    "\n",
    "    # Multiply by aj * 1{wx_plus_b > 0}\n",
    "    term1_b = (a * relu_mask)  # Shape: (n, m)\n",
    "    # Full sum: sum term1_b multiplied with appropriate term2, over i\n",
    "    full_sum_b = np.sum(term1_b * term2[:, np.newaxis], axis=0)  # Shape: (m,)\n",
    "\n",
    "    # Scale by constants\n",
    "    grad_a = -full_sum_a / (n * alpha_m)  # Final shape: (m,)\n",
    "    grad_w = -full_sum_w / (n * alpha_m)  # Final shape: (m,)\n",
    "    grad_b = -full_sum_b / (n * alpha_m)  # Final shape: (m,)\n",
    "\n",
    "    return grad_a, grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(step_size, steps, m, symmetric=False):\n",
    "\n",
    "    alpha_m, beta_1m, beta_2m = generate_alpha_betas(m)\n",
    "\n",
    "    m_len = math.ceil(m / 2) if symmetric else math.ceil(m)    \n",
    "\n",
    "    a = np.random.normal(loc=0, scale=beta_1m, size=m_len)\n",
    "    w = np.random.normal(loc=0, scale=beta_2m, size=m_len)\n",
    "    b = np.random.normal(loc=0, scale=beta_2m, size=m_len)\n",
    "\n",
    "    if symmetric:\n",
    "        a = np.concatenate((a, a))\n",
    "        w = np.concatenate((w, -w))\n",
    "        b = np.concatenate((b, b))\n",
    "\n",
    "    a_values = []\n",
    "    w_values = []\n",
    "    b_values = []\n",
    "\n",
    "    #for n in range(math.ceil(time/step_size)):\n",
    "    for n in range(steps): \n",
    "        grad_a, grad_w, grad_b = gradient_flow(a,w,b,alpha_m)\n",
    "        a = a + step_size * grad_a\n",
    "        w = w + step_size * grad_w\n",
    "        b = b + step_size * grad_b\n",
    "\n",
    "        # Save the current values of a, w, and b\n",
    "        a_values.append(a.copy())\n",
    "        w_values.append(w.copy())\n",
    "        b_values.append(b.copy())\n",
    "\n",
    "    return a_values, w_values, b_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(m_values, n_steps, learning_rate):\n",
    "    for m in m_values:\n",
    "        a_values, w_values, b_values = gradient_descent(learning_rate, n_steps, m, True)\n",
    "\n",
    "        # Convert lists to arrays for easier saving and plotting\n",
    "        a_values = np.array(a_values)\n",
    "        w_values = np.array(w_values)\n",
    "        b_values = np.array(b_values)\n",
    "\n",
    "        # Save the arrays to disk\n",
    "        np.save('weights\\\\a_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps), a_values)\n",
    "        np.save('weights\\\\w_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps), w_values)\n",
    "        np.save('weights\\\\b_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps), b_values)\n",
    "\n",
    "        print(f'Completed training for steps: {n_steps}, learning rate: {learning_rate}, m value: {m}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Cell generating min and max y values over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to plot the values at a given index\n",
    "def get_values(x_graph, a_vals, w_vals, b_vals, alpha_m, index):\n",
    "    a = a_vals[index]\n",
    "    w = w_vals[index]\n",
    "    b = b_vals[index]\n",
    "    \n",
    "    y_graph = np.zeros((int)(400)) # TODO: Try to remove this, initialising it each time is slow\n",
    "\n",
    "    # Compute the corresponding y values\n",
    "    for i in range(len(x_graph)):\n",
    "        y_graph[i] = twoLayerReluNet(alpha_m,a,w,b,x_graph[i])\n",
    "\n",
    "    return np.min(y_graph), np.max(y_graph)\n",
    "\n",
    "def gen_or_load_y_max_min_pts(x_graph, m, learning_rate, n_steps, index_refinment):\n",
    "    y_min_file = 'graphing_values\\\\y_max_min\\\\y_min_m%d_Learning%d_Steps%d_Refinment%d.npy' % (m, learning_rate, n_steps, index_refinment)\n",
    "    y_max_file = 'graphing_values\\\\y_max_min\\\\y_max_m%d_Learning%d_Steps%d_Refinment%d.npy' % (m, learning_rate, n_steps, index_refinment)\n",
    "\n",
    "    if os.path.exists(y_min_file) and os.path.exists(y_max_file):\n",
    "        print(f'Loading min and max values for m={m}, learning rate={learning_rate}, steps={n_steps}, refinement={index_refinment}')\n",
    "        return np.load(y_min_file), np.load(y_max_file)\n",
    "\n",
    "    # Load the arrays from disk\n",
    "    a_values = np.load('weights\\\\a_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps))\n",
    "    w_values = np.load('weights\\\\w_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps))\n",
    "    b_values = np.load('weights\\\\b_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps))\n",
    "\n",
    "    alpha_m = generate_alpha_betas(m)[0]\n",
    "\n",
    "    min_y_values = np.zeros((int)(n_steps/index_refinment))\n",
    "    max_y_values = np.zeros((int)(n_steps/index_refinment))\n",
    "    # Load min and max values obtained by graph at index\n",
    "    for n in range((int)(n_steps/index_refinment)):\n",
    "        min_y_values[n], max_y_values[n] = get_values(x_graph, a_values, w_values, b_values, alpha_m, n*index_refinment)\n",
    "\n",
    "    # Save the arrays to disk\n",
    "    np.save(y_min_file, min_y_values)\n",
    "    np.save(y_max_file, max_y_values)\n",
    "\n",
    "    return min_y_values, max_y_values\n",
    "\n",
    "def generate_y_max_min_plot(m_values, n_steps, learning_rate, index_refinment, scaling_func):\n",
    "\n",
    "    scale_text = inspect.getsource(scaling_func).strip()\n",
    "    scale_text = scale_text[scale_text.find(\":\")+2:-1]\n",
    "\n",
    "    # There could be too many points, so we only take every index_refinment points when needed to increase speed\n",
    "    # About 1 minute for 200k steps, refinement of 50 and m_values = [200, 400, 600, 800, 1000]\n",
    "\n",
    "    # x values for the graph\n",
    "    x_graph = np.linspace(-0.5, 0.5, 400)\n",
    "\n",
    "    # Create an array of indices\n",
    "    indices = np.arange(n_steps/index_refinment)\n",
    "\n",
    "    # Generate a colormap\n",
    "    colormap = cm.get_cmap('viridis', len(m_values))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for idx, m in enumerate(m_values):\n",
    "        min_y_values, max_y_values = gen_or_load_y_max_min_pts(x_graph, m, learning_rate, n_steps, index_refinment)\n",
    "\n",
    "        # Plot the min and max values\n",
    "        plt.plot(indices*scaling_func(m), min_y_values, color=colormap(idx))\n",
    "        plt.plot(indices*scaling_func(m), max_y_values, color=colormap(idx))\n",
    "\n",
    "        # Attach label\n",
    "        plt.plot([], [], color=colormap(idx), label=f'm = {m}')\n",
    "\n",
    "    filename = f'YMinMax_Scale{scale_text}_M{m_values}_Learning{learning_rate}_Steps{n_steps}.svg'\n",
    "\n",
    "    # Sanitize the file name by replacing invalid characters\n",
    "    invalid_chars = ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']\n",
    "    for char in invalid_chars:\n",
    "        filename = filename.replace(char, \"_\")\n",
    "\n",
    "    folder_path = f'figs\\\\y_min_max\\\\learning{learning_rate}\\\\'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(f'Index x{index_refinment} with scaling {scale_text}')\n",
    "    plt.ylabel('y values')\n",
    "    plt.title('Min and Max y values at index') # TODO: Recheck the text for labels and title\n",
    "    plt.xlim(0, 1e-5)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(folder_path+filename)\n",
    "    plt.show()\n",
    "    print(f'Completed plotting for steps: {n_steps}, learning rate: {learning_rate}, m values: {m_values}, rescale: {scale_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to plot the values at a given index\n",
    "def get_risk(a_vals, w_vals, b_vals, alpha_m, index):\n",
    "    a = a_vals[index]\n",
    "    w = w_vals[index]\n",
    "    b = b_vals[index]\n",
    "    \n",
    "    return network_risk(alpha_m, a, w, b, x, y)\n",
    "\n",
    "def gen_or_load_risk_pts(m, learning_rate, n_steps, index_refinment):\n",
    "    risk_file = 'graphing_values\\\\risk\\\\risk_m%d_Learning%d_Steps%d_Refinment%d.npy' % (m, learning_rate, n_steps, index_refinment)\n",
    "\n",
    "    if os.path.exists(risk_file):\n",
    "        print(f'Loading risk values for m={m}, learning rate={learning_rate}, steps={n_steps}, refinement={index_refinment}')\n",
    "        return np.load(risk_file)\n",
    "\n",
    "    # Load the arrays from disk\n",
    "    a_values = np.load('weights\\\\a_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps))\n",
    "    w_values = np.load('weights\\\\w_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps))\n",
    "    b_values = np.load('weights\\\\b_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps))\n",
    "\n",
    "    alpha_m = generate_alpha_betas(m)[0]\n",
    "\n",
    "    risk = np.zeros((int)(n_steps/index_refinment))\n",
    "    # Load min and max values obtained by graph at index\n",
    "    for n in range((int)(n_steps/index_refinment)):\n",
    "        risk[n] = get_risk(a_values, w_values, b_values, alpha_m, n*index_refinment)\n",
    "\n",
    "    # Save the arrays to disk\n",
    "    np.save(risk_file, risk)\n",
    "\n",
    "    return risk\n",
    "\n",
    "def generate_risk_plot(m_values, n_steps, learning_rate, index_refinment, scaling_func):\n",
    "\n",
    "    scale_text = inspect.getsource(scaling_func).strip()\n",
    "    scale_text = scale_text[scale_text.find(\":\")+2:-1]\n",
    "\n",
    "    # There could be too many points, so we only take every index_refinment points when needed to increase speed\n",
    "    # About 1 minute for 200k steps, refinement of 50 and m_values = [200, 400, 600, 800, 1000]\n",
    "\n",
    "    # Create an array of indices\n",
    "    indices = np.arange(n_steps/index_refinment)\n",
    "\n",
    "    # Generate a colormap\n",
    "    colormap = cm.get_cmap('viridis', len(m_values))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for idx, m in enumerate(m_values):\n",
    "        risk = gen_or_load_risk_pts(m, learning_rate, n_steps, index_refinment)\n",
    "\n",
    "        # Plot the min and max values\n",
    "        plt.plot(indices*scaling_func(m), risk, color=colormap(idx))\n",
    "\n",
    "        # Attach label\n",
    "        plt.plot([], [], color=colormap(idx), label=f'm = {m}')\n",
    "\n",
    "    filename = f'RiskPlot_Scale{scale_text}_M{m_values}_Learning{learning_rate}_Steps{n_steps}.svg'\n",
    "\n",
    "    # Sanitize the file name by replacing invalid characters\n",
    "    invalid_chars = ['/', '\\\\', ':', '*', '?', '\"', '<', '>', '|']\n",
    "    for char in invalid_chars:\n",
    "        filename = filename.replace(char, \"_\")\n",
    "\n",
    "    folder_path = f'figs\\\\risk\\\\learning{learning_rate}\\\\'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(f'Index x{index_refinment} with scaling {scale_text}')\n",
    "    plt.ylabel('risk')\n",
    "    plt.xlim(0, 1e-5)\n",
    "    plt.ylim(1e-4, 1e-1)\n",
    "    plt.title('Risk at index') # TODO: Recheck the text for labels and title\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(folder_path+filename)\n",
    "    plt.show()\n",
    "    print(f'Completed plotting for steps: {n_steps}, learning rate: {learning_rate}, m values: {m_values}, rescale: {scale_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and save all weights and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training for steps: 200000, learning rate: 70, m value: 800\n"
     ]
    }
   ],
   "source": [
    "def gen_graphs(m_values, n_steps, learning_rate, index_refinment):\n",
    "    generate_y_max_min_plot(m_values, n_steps, learning_rate, index_refinment, lambda m: 1)\n",
    "    generate_y_max_min_plot(m_values, n_steps, learning_rate, index_refinment, lambda m: (1/m))\n",
    "    generate_y_max_min_plot(m_values, n_steps, learning_rate, index_refinment, lambda m: (1/m)**2)\n",
    "    generate_y_max_min_plot(m_values, n_steps, learning_rate, index_refinment, lambda m: (1/math.log(m)))\n",
    "\n",
    "    generate_risk_plot(m_values, n_steps, learning_rate, index_refinment, lambda m: 1)\n",
    "    generate_risk_plot(m_values, n_steps, learning_rate, index_refinment, lambda m: (1/m))\n",
    "    generate_risk_plot(m_values, n_steps, learning_rate, index_refinment, lambda m: (1/m)**2)\n",
    "    generate_risk_plot(m_values, n_steps, learning_rate, index_refinment, lambda m: (1/math.log(m)))\n",
    "\n",
    "m_values = np.array([800])\n",
    "\n",
    "## Learning rate of 4000\n",
    "\n",
    "n_steps = 200000\n",
    "learning_rate = 70\n",
    "train(m_values, n_steps, learning_rate)\n",
    "# gen_graphs(m_values, n_steps, learning_rate, 25)\n",
    "\n",
    "## Learning rate of 2000\n",
    "\n",
    "# n_steps = 250000\n",
    "# learning_rate = 2000\n",
    "# # train(m_values, n_steps, learning_rate)\n",
    "# # gen_graphs(m_values, n_steps, learning_rate, 25)\n",
    "\n",
    "\n",
    "# ## Learning rate of 1000\n",
    "\n",
    "# n_steps = 300000\n",
    "# learning_rate = 1000\n",
    "# # train(m_values, n_steps, learning_rate)\n",
    "# # gen_graphs(m_values, n_steps, learning_rate, 25)\n",
    "\n",
    "\n",
    "# ## Learning rate of 4000 but very small m\n",
    "\n",
    "# m_values = [40, 60, 80, 100, 120]\n",
    "# n_steps = 2200\n",
    "# learning_rate = 4000\n",
    "# train(m_values, n_steps, learning_rate)\n",
    "# gen_graphs(m_values, n_steps, learning_rate, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Cell generating function development over time - Used for testing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2114e7e9373422d870dfb5e2379619e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Index', max=199999), Output()), _dom_classes=('widget-inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e43f1b212544086813b049db52ac5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider, Dropdown, VBox\n",
    "\n",
    "# Define the range of x\n",
    "x_graph = np.linspace(-0.5, 0.5, 800)\n",
    "\n",
    "# Global variables to hold the current state\n",
    "current_m = None\n",
    "current_a_values = None\n",
    "current_w_values = None\n",
    "current_b_values = None\n",
    "\n",
    "# Output widget for displaying the plot\n",
    "output = Output()\n",
    "\n",
    "m_values = [800]\n",
    "# n_steps = 2200\n",
    "#learning_rate = 10\n",
    "\n",
    "def load_data_for_m(m):\n",
    "    \"\"\"Load data for a specific m and keep only the current m's data in memory.\"\"\"\n",
    "    global current_m, current_a_values, current_w_values, current_b_values\n",
    "\n",
    "    # If m has not changed, skip loading\n",
    "    if m == current_m:\n",
    "        return\n",
    "\n",
    "    # Load new data for m\n",
    "    current_a_values = np.load('weights\\\\a_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps))\n",
    "    current_w_values = np.load('weights\\\\w_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps))\n",
    "    current_b_values = np.load('weights\\\\b_values_m%d_Learning%d_Steps%d.npy' % (m, learning_rate, n_steps))\n",
    "    current_m = m\n",
    "\n",
    "def plot_values(index):\n",
    "    \"\"\"Plot the function for the selected m and index.\"\"\"\n",
    "    # Ensure data for the selected m is loaded\n",
    "    m = m_values[0]\n",
    "    load_data_for_m(m)\n",
    "    a = current_a_values[index]\n",
    "    w = current_w_values[index]\n",
    "    b = current_b_values[index]\n",
    "\n",
    "    alpha_m = generate_alpha_betas(m)[0]\n",
    "\n",
    "    # Create the main plot\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Compute the corresponding y values\n",
    "    y_graph = np.array([twoLayerReluNet(alpha_m, a, w, b, x) for x in x_graph])\n",
    "\n",
    "     # Clear the previous plot and create a new one\n",
    "    with output:\n",
    "        output.clear_output(wait=True)\n",
    "        ax1.scatter(-b/w, a, c='blue',marker=(5, 1))\n",
    "        ax1.axhline(y=0, color='blue', linestyle='--', linewidth=0.5)\n",
    "        ax1.set_ylim(-3,30)\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.scatter(x, y, c='orange', alpha=0.5, label='Data points')\n",
    "        ax2.plot(x_graph, y_graph)\n",
    "        ax2.grid(True)\n",
    "        ax2.set_xlim(-2, 2)\n",
    "        ax2.set_ylim(-0.01, 0.3)\n",
    "        plt.show()\n",
    "\n",
    "# Create a slider for selecting the index\n",
    "slider = IntSlider(min=0, max=n_steps-1, step=1, value=0, description='Index')\n",
    "\n",
    "# Use interact to create an interactive plot\n",
    "interact(plot_values, index=slider)\n",
    "\n",
    "# Display the output widget\n",
    "display(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
