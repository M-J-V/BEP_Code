{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Saddle-to-Saddle Dynamics in Diagonal Linear Networks<br>\n",
    "*Pesme, S., & Flammarion, N. (2024). Saddle-to-saddle dynamics in diagonal linear networks*. Journal of Statistical Mechanics Theory and Experiment, 2024(10), 104016. https://doi.org/10.1088/1742-5468/ad65e3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "import math\n",
    "from itertools import product, combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation and Parameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2 # Dimensions of the input vectors\n",
    "n = 2 # Number of samples\n",
    "\n",
    "# Initial values of the weights \n",
    "# alpha = 1e-2\n",
    "# v_0 = np.array([0 for x in range(d)])\n",
    "# u_0 = math.sqrt(2)*alpha*np.array([1 for x in range(d)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a dataset for our use. The method of generation is taken from Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7, 1.5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_dataset(n, d):\n",
    "    # Generate the random parameters for the dataset\n",
    "    H = np.diag(np.random.rand(d))  # Random diagonal entries for covariance matrix\n",
    "    beta_star = np.random.randn(d) # Initialize beta^* as a random vector in R^d\n",
    "    \n",
    "    # Generate the dataset\n",
    "    x = np.random.multivariate_normal(np.zeros(d), H, n) # Generate x_i from multivariate normal distribution with mean 0 and covariance H\n",
    "    y = np.dot(x, beta_star)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Generate the dataset for fixed H and beta_star\n",
    "def generate_dataset_fixed_H_beta(n,d,H,beta_star):\n",
    "    # Generate the dataset\n",
    "    x = np.random.multivariate_normal(np.zeros(d), H, n) # Generate x_i from multivariate normal distribution with mean 0 and covariance H\n",
    "    y = np.dot(x, beta_star)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# samples_x, samples_y = generate_dataset(n, d)\n",
    "# For the sake of consistent results when testing, we will fix the dataset\n",
    "samples_x = np.array([[1, 0.6], [0.2, 0.8]])\n",
    "beta = np.array([-0.5, 2])\n",
    "samples_y = samples_x @ beta\n",
    "\n",
    "samples_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Important Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-layer Diagonal Linear Network\n",
    "$ x \\mapsto \\langle u, \\sigma(diag(v)x)\\rangle $ <br> \n",
    "$x \\mapsto \\langle \\beta, x\\rangle$ where $\\beta = u \\odot v$\n",
    "\n",
    "$u$ - Output weights <br> \n",
    "$\\sigma$ - Identity function<br> \n",
    "$\\text{diag}(v)$ - Inner weights<br> \n",
    "\n",
    "\n",
    "### Quadratic Loss Function\n",
    "$L(\\beta) = \\frac{1}{2n} \\sum_{i=1}^{n} (\\langle \\beta, x_i\\rangle - y_i)^2$<br>\n",
    "$F(\\omega) := L(u \\odot v)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Layer Diagonal Linear Network\n",
    "def twolDLN(u,v,x):\n",
    "    return np.inner(u*v, x) # We use the regression vector here for conciseness\n",
    "\n",
    "# Quadratic Loss Function\n",
    "def loss(u,v):\n",
    "    return (1/2*n)*sum([(twolDLN(u,v,samples_x[i])-samples_y[i])**2 for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential $\\phi_\\alpha$ - Hyperbolic Entropy function:\n",
    "$\\phi_\\alpha(\\Beta) = \\frac{1}{2} \\sum^d_{i=1}\\left( \\Beta_i \\sinh^{-1}(\\frac{\\Beta_i}{\\alpha^2} - \\sqrt{\\Beta_i^2 +\\alpha^4} + \\alpha^2)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential(beta, alpha):\n",
    "    # Ensure beta is a numpy array\n",
    "    beta = np.array(beta)\n",
    "    \n",
    "    # Calculate the function for each beta_i\n",
    "    result = 1/2*np.sum([beta[i] * np.arcsinh(beta[i] / alpha**2) - np.sqrt(beta[i]**2 + alpha**4) + alpha**2 for i in range(d)])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gradient Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimise loss $F$ (without accelerating time)\n",
    "\n",
    "$\\text{d}w_t = - \\nabla F(w_t)\\text{d}t$\n",
    "\n",
    "For Gradient descent:\n",
    "$w_{t+1} = w_t - \\eta \\nabla F(w_t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation $\\nabla F$\n",
    "Consider for now only $u$\n",
    "\n",
    "$\\frac{\\partial F}{\\partial u} = \\frac{1}{2n} \\sum_{i=1}^n 2(\\langle u \\odot v,x_i \\rangle - y_i) \\cdot \\frac{\\partial}{\\partial u}\\langle u \\odot v, x_i \\rangle$\n",
    "\n",
    "Since $\\langle u \\odot v, x_i\\rangle = \\sum_{j=1}^d (u_j v_j x_{ij})$ then $\\frac{\\partial}{\\partial u_j}\\langle u \\odot v, x_i\\rangle = (v_j x_{ij})$\n",
    "\n",
    "Therefore\n",
    "$\\frac{\\partial F}{\\partial u} = \\frac{1}{n} \\sum_{i=1}^n (\\langle u \\odot v,x_i \\rangle - y_i) \\cdot (v \\odot x_i )$\n",
    "\n",
    "Similarly \n",
    "$\\frac{\\partial F}{\\partial v} = \\frac{1}{n} \\sum_{i=1}^n (\\langle u \\odot v,x_i \\rangle - y_i) \\cdot (u \\odot x_i )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Derivative of the Loss Function with respect to w\n",
    "def nabla_F(u,v):\n",
    "    nabla_F_u = 1/n * sum([(twolDLN(u,v,samples_x[i])-samples_y[i])*(v*samples_x[i]) for i in range(n)])\n",
    "    nabla_F_v = 1/n * sum([(twolDLN(u,v,samples_x[i])-samples_y[i])*(u*samples_x[i]) for i in range(n)])\n",
    "    return nabla_F_u, nabla_F_v, np.concatenate((nabla_F_u, nabla_F_v))\n",
    "\n",
    "# Gradient Descent\n",
    "def gradient_descent(u,v,eta):\n",
    "    nabla_F_u, nabla_F_v, _ = nabla_F(u,v)\n",
    "    u = u - eta*nabla_F_u\n",
    "    v = v - eta*nabla_F_v\n",
    "    return u, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising the Gradient Flow\n",
    "\n",
    "We are interested in trying to observe the saddle-to-saddle movements that we may expect.\n",
    "$\\beta$ is the parameter that we pay attention to\n",
    "\n",
    "Things that we expect to notice:\n",
    "1. As the $\\alpha$ approaches 0, the time required to notice the saddle 'jumps' increases (the time taken for a jump remains the same)\n",
    "1. When $\\alpha$ is very close to 0, then $\\Beta$ remains at the origin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea26ca0db93447da7acfb2031e1bd27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=200, description='Time', max=1000, min=50, step=50), FloatSlider(value=0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_gradient_descent(time=200, eta=0.1, alpha=0.1)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following code is used to plot the gradient descent path of beta over time for the 2-layer diagonal linear network.\n",
    "It will only work for d=2, as the plot is in 2D.\n",
    "\"\"\"\n",
    "\n",
    "def plot_gradient_descent(time=200,eta=0.1,alpha=1e-1):\n",
    "    # Number of steps for the gradient descent1\n",
    "    n_steps = math.floor(time / eta)\n",
    "\n",
    "    # Initial values of the weights\n",
    "    v_0 = np.array([0 for x in range(d)])\n",
    "    u_0 = math.sqrt(2)*alpha*np.array([1 for x in range(d)])\n",
    "\n",
    "    # Gradient Descent\n",
    "    u = u_0\n",
    "    v = v_0\n",
    "    u_values = [u] # Store the values of u for each iteration\n",
    "    v_values = [v] # Store the values of v for each iteration\n",
    "    beta_values = [v*u] # Store the values of beta for each iteration\n",
    "    potential_values = [potential(v*u, alpha)] # Store the values of the potential for each iteration # TODO: Check if this is at all relevant\n",
    "\n",
    "    # Perform the gradient descent\n",
    "    for _ in range(n_steps):\n",
    "        u, v = gradient_descent(u,v,eta)\n",
    "        u_values.append(u.copy()) # Save values for plotting\n",
    "        v_values.append(v.copy()) # Save values for plotting\n",
    "        beta_values.append(v*u)\n",
    "        potential_values.append(potential(v*u, alpha))\n",
    "\n",
    "    # The if statement is simply to hide bulky graphing code :)\n",
    "    if(True):\n",
    "        # Convert u_values to a numpy array for easier plotting\n",
    "        u_values = np.array(u_values)\n",
    "        v_values = np.array(v_values)\n",
    "        beta_values = np.array(beta_values)\n",
    "        time_values = np.array([step*eta for step in range(n_steps+1)])\n",
    "        # Create a figure and two subplots (1 row, 2 columns)\n",
    "        plt.figure(figsize=(24, 6))\n",
    "\n",
    "        # First subplot: Gradient descent path for beta[0]\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(time_values, beta_values[:, 0], s=0.2, c='purple')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Beta[0]')\n",
    "        plt.title(f'Beta[0] over time (eta = {round(eta,5)}, alpha = {round(alpha,5)})')\n",
    "        plt.grid(True)\n",
    "        plt.grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "        plt.minorticks_on()\n",
    "\n",
    "        # Second subplot: Gradient descent path for beta[0]\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(time_values, beta_values[:, 1], s=0.2, c='purple')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Beta[1]')\n",
    "        plt.title(f'Beta[1] over time (eta = {round(eta,5)}, alpha = {round(alpha,5)})')\n",
    "        plt.grid(which='both')\n",
    "        plt.grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "        plt.minorticks_on()\n",
    "\n",
    "        plt.savefig(f'plots/BetaPlots_{time}_{eta}_{alpha}.png')\n",
    "\n",
    "    # Add sliders to control the parameters\n",
    "interact(\n",
    "    plot_gradient_descent,\n",
    "    time=IntSlider(value=200, min=50, max=1000, step=50, description=\"Time\"),\n",
    "    eta=FloatSlider(value=0.01, min=0.001, max=0.1, step=0.001, description=\"Learning Rate\",readout_format='.3f'),\n",
    "    alpha=FloatSlider(value=0.001, min=0.0001, max=0.1, step=0.0001, description=\"Alpha\",readout_format='.4f'),\n",
    ")\n",
    "\n",
    "    # # Create a figure and two subplots (1 row, 2 columns)\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # # First subplot: Gradient descent path for u\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.scatter(u_values[:, 0], u_values[:, 1], s=0.2, c='blue', label='Gradient Descent Path of u')\n",
    "    # plt.scatter(u_values[0, 0], u_values[0, 1], c='yellow', label='Initial Value of u', edgecolors='black')\n",
    "    # plt.scatter(u_values[-1, 0], u_values[-1, 1], c='green', label='Final Value of u', edgecolors='black')\n",
    "    # plt.xlabel('u[0]')\n",
    "    # plt.ylabel('u[1]')\n",
    "    # plt.title(f'Gradient Descent Path for u (eta = {eta}, alpha = {alpha})')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "\n",
    "    # # Second subplot: Gradient descent path for v\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.scatter(v_values[:, 0], v_values[:, 1], s=0.2, c='red', label='Gradient Descent Path for v')\n",
    "    # plt.scatter(v_values[0, 0], v_values[0, 1], c='yellow', label='Initial Value of v', edgecolors='black')\n",
    "    # plt.scatter(v_values[-1, 0], v_values[-1, 1], c='green', label='Final Value of v', edgecolors='black')\n",
    "    # plt.xlabel('v[0]')\n",
    "    # plt.ylabel('v[1]')\n",
    "    # plt.title(f'Gradient Descent Path for v (eta = {eta}, alpha = {alpha})')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "\n",
    "    # # Adjust the layout to avoid overlap\n",
    "    # plt.tight_layout()\n",
    "    # # Display the plots\n",
    "    # plt.show()\n",
    "\n",
    "    # # Assuming u_values, v_values, and beta_values are already defined as numpy arrays\n",
    "\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # # Plot Gradient descent path for u\n",
    "    # plt.scatter(u_values[:, 0], u_values[:, 1], s=0.2, c='blue', label='Gradient Descent Path of u')\n",
    "    # plt.scatter(u_values[0, 0], u_values[0, 1], c='yellow', label='Initial Value of u', edgecolors='black')\n",
    "    # plt.scatter(u_values[-1, 0], u_values[-1, 1], c='green', label='Final Value of u', edgecolors='black')\n",
    "\n",
    "    # # Plot Gradient descent path for v\n",
    "    # plt.scatter(v_values[:, 0], v_values[:, 1], s=0.2, c='red', label='Gradient Descent Path for v')\n",
    "    # plt.scatter(v_values[0, 0], v_values[0, 1], c='yellow', label='Initial Value of v', edgecolors='black')\n",
    "    # plt.scatter(v_values[-1, 0], v_values[-1, 1], c='green', label='Final Value of v', edgecolors='black')\n",
    "\n",
    "    # # Add labels, title, legend, and grid\n",
    "    # plt.xlabel('Value[0]')\n",
    "    # plt.ylabel('Value[1]')\n",
    "    # plt.title(f'Gradient Descent Paths for u, v, and beta (eta = {eta}, alpha = {alpha})')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "\n",
    "    # # Display the plot\n",
    "    # plt.show()\n",
    "\n",
    "    # # Fourth subplot: potential function\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.plot(potential_values, c='orange', label='Potential Function')\n",
    "    # plt.xlabel('Iteration')\n",
    "    # plt.ylabel('Poterntial Function Value')\n",
    "    # plt.title(f'Potentiall Function (eta = {eta}, alpha = {alpha})')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Rescaling for the potential\n",
    "\n",
    "$\\tilde{\\phi}_\\alpha := \\frac{1}{\\ln(1/\\alpha)} \\phi_\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_potential(beta, alpha):\n",
    "\n",
    "    # Rescale the hyperbolic entropy\n",
    "    result = 1/(math.log(1/alpha)) * potential(beta, alpha)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beta_value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m scaled_potential_values \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# Store the values of the hyperbolic entropy for each iteration\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m beta \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbeta_value\u001b[49m:\n\u001b[0;32m      3\u001b[0m     scaled_potential_values\u001b[38;5;241m.\u001b[39mappend(scaled_potential(beta, alpha))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create a figure and two subplots (1 row, 2 columns)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'beta_value' is not defined"
     ]
    }
   ],
   "source": [
    "scaled_potential_values = [] # Store the values of the hyperbolic entropy for each iteration\n",
    "for beta in beta_value:\n",
    "    scaled_potential_values.append(scaled_potential(beta, alpha))\n",
    "\n",
    "# Create a figure and two subplots (1 row, 2 columns)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# First subplot: Gradient descent path for beta\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(beta_value[:, 0], beta_value[:, 1], s=0.2, c='purple', label='Gradient Descent Path for beta')\n",
    "plt.scatter(beta_value[0, 0], beta_value[0, 1], c='yellow', label='Initial Value of beta', edgecolors='black')\n",
    "plt.scatter(beta_value[-1, 0], beta_value[-1, 1], c='green', label='Final Value of beta', edgecolors='black')\n",
    "plt.xlabel('beta[0]')\n",
    "plt.ylabel('beta[1]')\n",
    "plt.title(f'Gradient Descent Path for beta (eta = {eta}, alpha = {alpha})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Second subplot: potential function\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(scaled_potential_values, c='orange', label='Scaled Potential Function')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Scaled Poterntial Function Value')\n",
    "plt.title(f'Scaled Potential Function (eta = {eta}, alpha = {alpha})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(Assumption 1) - General Positions**\n",
    "\n",
    "For any $k \\leq \\min(n,d)$ and arbitrary signs $\\sigma_1, \\dots, \\sigma_k \\in \\{-1,1\\}$, the affine span of any $k$ points $\\sigma \\tilde{x}_{j_1}, \\ldots, \\sigma \\tilde{x}_{j_k}$ does not contain any element of the set $\\{ \\pm \\tilde{x_j}, j \\neq j_1, \\ldots, j_k\\}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_affine_span(X, k):\n",
    "    \"\"\"\n",
    "    Check if the dataset X satisfies the general position assumption.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy array): Dataset with shape (n, d), where n is the number of samples and d is the dimensionality.\n",
    "                     The ith column of X corresponds to the feature vector x_̃j in the assumption.\n",
    "    k (int): Number of points to check in the affine span. Must be <= min(n, d).\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the assumption holds, False otherwise.\n",
    "\n",
    "    Warning: \n",
    "    This function has exponential complexity in d and k. It is only suitable for small values of d and k.\n",
    "    \"\"\"\n",
    "    # Generate all combinations of k columns and their corresponding sign combinations\n",
    "    for indices in combinations(range(d), k): # all combinations of k columns\n",
    "        for signs in product([-1, 1], repeat=k): # all sign combinations\n",
    "            # Select k columns (corresponding to features) and apply sign changes\n",
    "            selected_columns = np.array([sign * X[:, idx] for sign, idx in zip(signs, indices)]).T  # Make array of all selected columns and their signs\n",
    "            \n",
    "            # Compute the affine span of the selected columns\n",
    "            # (Affine span is equivalent to checking linear independence after centering the points)\n",
    "            centered_columns = selected_columns - selected_columns.mean(axis=0) #Centering is necessary for checking the #affine# span not just the linear span\n",
    "            if np.linalg.matrix_rank(centered_columns) < k:\n",
    "                continue  # If points are linearly dependent, skip this combination\n",
    "            \n",
    "            # Now check if any other columns lie in the affine span\n",
    "            remaining_indices = [i for i in range(d) if i not in indices] # Columns not in the selected set\n",
    "            for idx in remaining_indices:\n",
    "                for sign in [-1, 1]:\n",
    "                    candidate_column = sign * X[:, idx]\n",
    "                    augmented_matrix = np.hstack([centered_columns, candidate_column[:, None] - selected_columns.mean(axis=0)])\n",
    "                    if np.linalg.matrix_rank(augmented_matrix) == k:\n",
    "                        # If the rank doesn't increase, the point lies in the affine span\n",
    "                        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General position assumption is satisfied for k =  1\n",
      "General position assumption is satisfied for k =  2\n"
     ]
    }
   ],
   "source": [
    "k = min(d, n)  # Minimum of d and n for general positions\n",
    "\n",
    "for i in range(1, k+1):\n",
    "    if not check_affine_span(samples_x, i):\n",
    "        print(\"General position assumption is not satisfied for k = \", i)\n",
    "    else:\n",
    "        print(\"General position assumption is satisfied for k = \", i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
